import os
import google.generativeai as genai
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

class LLMClient:
    _instance = None

    def __new__(cls):
        if cls._instance is None:
            cls._instance = super(LLMClient, cls).__new__(cls)
            cls._instance._initialize()
        return cls._instance

    def _initialize(self):
        self.api_key = os.getenv("LLM_API_KEY")
        if not self.api_key:
            raise ValueError("LLM_API_KEY environment variable not set. Please set it in the .env file.")

        genai.configure(api_key=self.api_key)
        
        # --- TEMPORARY DEBUGGING CODE ---
        print("\n--- Available Generative Models ---")
        for m in genai.list_models():
            if 'generateContent' in m.supported_generation_methods:
                print(f"Model: {m.name}, Methods: {m.supported_generation_methods}")
        print("-----------------------------------\n")
        # --- END TEMPORARY DEBUGGING CODE ---

        # Using a generative model that supports text generation
        # Adjust model name if a different one is preferred or available
        self.model = genai.GenerativeModel('models/gemini-flash-latest') 

    async def generate_summary(self, text: str, prompt_instruction: str) -> str:
        """
        Generates a summary using the configured LLM.
        """
        full_prompt = f"{prompt_instruction}\n\nText to summarize:\n{text}"
        try:
            response = self.model.generate_content(full_prompt)
            if response and response.candidates and response.candidates[0].content and response.candidates[0].content.parts:
                return response.candidates[0].content.parts[0].text
            else:
                # Add more specific error for empty response from LLM
                print(f"LLM returned no content for prompt: {full_prompt}")
                return "No summary generated by LLM."
        except Exception as e:
            print(f"Error generating summary from LLM: {e}")
            raise

llm_client = LLMClient()
